{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as ps\nimport tensorflow as tf\n\n\"\"\"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport tensorflow_hub as hub\n\n\n# Reproductability\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\n\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '../input/garbagedataset/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\nds_valid_ = image_dataset_from_directory(\n    '../input/garbagedataset/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Early Stopping - Also set callbacks from 'history'\nearly_stopping = EarlyStopping(\n   min_delta=0.03, # minimium amount of change to count as an improvement\n   patience=5, # how many epochs to wait before stopping\n   restore_best_weights=True,\n)\nprint('Early_Stopping Set')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.Sequential([\n    layers.InputLayer(input_shape=[128, 128, 3]),\n    \n    # Data Augmentation\n    preprocessing.RandomContrast(factor=0.30),\n    preprocessing.RandomFlip(mode='horizontal'),\n    preprocessing.RandomRotation(factor=0.30),\n    \n    # Block One\n    layers.BatchNormalization(renorm=True),\n    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Block Two\n    layers.BatchNormalization(renorm=True),\n    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Block Three\n    layers.BatchNormalization(renorm=True),\n    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Head\n    layers.BatchNormalization(renorm=True),\n    layers.Flatten(),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(1, activation='sigmoid'),\n])\nprint('complete')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(epsilon=0.01)\nmodel.compile( \n    optimizer=optimizer, \n    loss='binary_crossentropy', # MAE or neg_MAE\n    metrics=['binary_crossentropy'],\n) \n\nhistory = model.fit( \n    ds_train, \n    validation_data=ds_valid,   \n    epochs=100, \n    #callbacks=[early_stopping] \n)\n\n# Plot learning curves\nimport pandas as pd \nhistory_frame = pd.DataFrame(history.history) \nhistory_frame.loc[:, ['loss', 'val_loss']].plot() \nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(); ","metadata":{},"execution_count":null,"outputs":[]}]}